{"cells":[{"cell_type":"markdown","source":["# Data Quality Spy\n","A Fabric notebook to scan for data quality issues in semantic models across workspaces in Fabric tenant. This version so far only looks at [referential integrity (RI) violations](https://dax.tips/2019/11/28/clean-data-faster-reports/) in semantic models.\n","\n","---\n","\n","What this notebook does:\n","1. takes a list of workspaces;\n","2. loops over semantic models in those workspaces to read tables and list RI violations in relationships between tables;\n","3. tries to capture errors for incompatible semantic models (e.g. auto-generated models like workspace Usage Metrics, default semantic model of lakehouse);\n","4. loads results to a Pandas DataFrame;\n","5. writes Pandas DataFrame to CSV file in lakehouse.\n","\n","---\n","\n","Room for improvement:\n","- refactor Python code (use dictionary comprehensions instead of loops where possible, more robust and verbose exception handling, verify PEP compliance).\n","- read list of workspaces from API response or file. Tutorial for API calls by Kurt Buhler: https://data-goblins.com/power-bi/power-bi-api-python.\n","- leverage Spark by using Spark DataFrames instead of combination of Fabric DataFrames and Pandas DataFrames.\n","- write results to files in batches to avoid memory issues when scanning many models.\n","\n","---\n","\n","Ideas for expansion:\n","- column statistics (data type, size, cardinality, missingness, variance) from DAX queries\n","- query DMVs if possible\n","---\n","\n","Last updated: 2023-11-25\n","\n","Author: Ruben Van de Voorde, revision by Max Van Goethem\n","\n","---"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"37d5795a-deae-47ad-b16a-2da94e0d4bb5"},{"cell_type":"markdown","source":["## Code"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ea94f35a-8e11-4b9c-98be-ae68aff831a8"},{"cell_type":"code","source":["# provide list of ids for workspaces to check semantic models in; can be found after https://app.powerbi.com/groups/ in url for workspace\n","# hardcoded here but possible to read from API response or file instead\n","wss = [\n","    '11111111-1111-1111-1111-111111111111',\n","    '22222222-2222-2222-2222-222222222222',\n","    '33333333-3333-3333-3333-333333333333'\n","]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9cab98c7-0ac1-4f96-8b35-9dd3d3d39ec1"},{"cell_type":"code","source":["# install dependencies\n","# consider installing semantic-link for workspace instead of %pip install\n","%pip install semantic-link\n","import pandas as pd\n","import datetime\n","import sempy.fabric as fabric\n","from sempy.fabric import list_relationship_violations"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}}},"id":"e42899ae-735f-4f72-851b-24ed3c3671e0"},{"cell_type":"code","source":["# loop over semantic models to read tables and list RI violations in relationships between tables and try to capture errors\n","ws_violations = []\n","for ws in wss:\n","    dss = fabric.list_datasets(ws)['Dataset Name']\n","    for ds in dss:\n","        try:\n","            tbls = {tbl: fabric.read_table(table=tbl, dataset=ds, workspace=ws)\n","            for tbl in fabric.list_tables(dataset=ds, workspace=ws)['Name']}\n","            ds_violations = fabric.list_relationship_violations(tbls)\n","            ds_violations['Workspace Id'] = ws\n","            ds_violations['Dataset Name'] = ds\n","            ds_violations['Timestamp'] = datetime.datetime.now()\n","            ds_violations['Error Status'] = 'No error'\n","            ws_violations.append(ds_violations)\n","        except Exception as e:\n","            # need to create schema for errors df in case first dataset checked returns errors\n","            ds_errors = pd.DataFrame(columns=['Multiplicity', 'From Table', 'From Column', 'To Table',\n","            'To Column', 'Type', 'Message', 'Workspace Id', 'Dataset Name', 'Timestamp', 'Error Status'])\n","            ds_errors['Message'] = [e]\n","            ds_errors['Workspace Id'] = ws\n","            ds_errors['Dataset Name'] = ds\n","            ds_errors['Timestamp'] = datetime.datetime.now()\n","            ds_errors['Error Status'] = 'Error'\n","            ws_violations.append(ds_errors)\n","\n","ri_violations = pd.concat(ws_violations)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"3a8200a6-d789-49a4-9080-16df5f130ac1"},{"cell_type":"code","source":["# print Pandas DataFrame for visual inspection\n","ri_violations"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"af9ef619-acfc-4c08-b5b8-843f86238bef"},{"cell_type":"code","source":["# write Pandas DataFrame to CSV in lakehouse\n","ri_violations.to_csv(\n","    # can write to non-attached lakehouse by replacing \"/lakehouse/default/\" with ABFFS path\n","    # ABFFS path can be found in Properties of 'Files' in lakehouse UI\n","    \"/lakehouse/default/\"\n","    \"/Files\" +\n","    \"/dataset-quality_ri-violations_\" +\n","    str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")) +\n","    \".csv\",\n","    index = False\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"6758528f-d97f-4810-8f8c-e357a4005e1d"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"token":"88ab27fe-d4d1-492f-b1e9-ef785e6a50b5","state":{}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"1759d3d0-5036-4fde-b623-93d241e1ede2"}],"default_lakehouse":"1759d3d0-5036-4fde-b623-93d241e1ede2","default_lakehouse_name":"AllSeeingLH","default_lakehouse_workspace_id":"659d6093-a90b-41e2-af35-8e6d772fcc59"}}},"nbformat":4,"nbformat_minor":5}